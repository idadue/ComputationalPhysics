\documentclass[../main.tex]{subfiles}

\begin{document}
\section{Discussion}\label{sec:discussion}

\subsection{Agreement with analytical results}

From \cref{tab:L2-numerical-vs-analytical}, we observe that our code achieved a reasonable agreement with the analytical values for the $L=2$ case with $T=1.0$. The heat capacity is the least in agreement for the maximum $10^6$ MC cycles, only conforming to the analytic value to one significant digit. The energy, magnetization and susceptibility all conform to at least 3 significant digits. However, the accuracy with $10^5$ MC cycles is not substantially worse, still agreeing to 3 significant digits fort everything but the heat capacity. Thus we calculate much of the subsequent values using $10^5$ MC cycles for expediency, confirming their stability when plotting their evolution through MC cycles in \cref{subsec:equilibration-time}.

\subsection{Estimation of equilibration time}

The equilibration time is a measure of how many cycles it takes before the system reduces to its equilibrium state. Assessing this visually for \cref{fig:equilibration-time}, it appears that no more than $2 \times 10^4$ MC cycles are required for the energy, absolute magnetization and heat capacity to stabilize. However, the magnetic susceptibility is clearly still varying substantially up to about $1 \times 10^5$ MC cycles. However, this is only true for the case of $\frac{k_B T}{J} = 2.4$. When $\frac{k_B T}{J} = 1.0$, the ground state is reached very quickly, at just about $1 \times 10^4$ MC cycles all thermodynamic quantities have stabilized with a randomized start. For the case of beginning with the ground state, the equilibration time appears to approach $0$ MC cycles, as all four quantities are stable during the entire simulation. We use a more conservative equilibration time of $10^4$ MC cycles in further calculations. Longer equilibration times will also reduce the amount of data gathered, but further work could be in automatically verifying the equilibration time, perhaps individually for each thermodynamic quantity.

\subsection{Accepted spin-flips}

The number of potential spin flips scales as $L^2 \times N$, where $N$ is the number of MC cycles. Thus we expect a linear graph when plotting the spin flips against MC cycles. In \cref{fig:spin-flips} we see that for $k_B T = 2.4$, this linear relationship is preserved. However, the lower temperature $k_B T = 1.0$ yields a graph that actually descends to fewer numbers of accepted spin-flips for $10^3$ MC cycles. This can be explained by the fact that the low-energy system tends to be in the ground state, with all spins pointing in the same direction. The transition probability to the state directly above the ground state for $\frac{k_B T}{J} = 1.0$ is $\exp(-8) \approx 0.00034$, and so for small numbers of MC cycles, we expect some variation. The low temperature graph does seem to follow a roughly linear ascent otherwise.

It is important to note that the ground state and the state just above the ground state has an energy difference of $8J$, as explained in \cref{tab:possible-delta-E}. This is visible as a gap in the energy distribution in \cref{fig:energy-distribution}, between $E = -800$ and $E = -792$. Other states tend to have at least some spins that can be changed to yield an energy gap of $4J$, which means that the ground state is particularly unlikely to accept all spin changes.

In \cref{fig:spin-flips-temp} we see the relationship between accepted states and temperature. The graph sharply rises near $\frac{k_B T}{J} = 2.25$, which happens to be the closest to the critical temperature that this resolution permits. It might be of interest as further work to create a graph like this but for larger lattices, to see if the same structure remains and whether it changes shape appreciably. 

\subsection{Probability distribution function}

The distribution of states by energy is expected to follow a Maxwell-Boltzmann distribution, as we are using \cref{eq:boltzmann-distribution} to determine the probability of transition through the canonical ensemble. Figure \ref{fig:energy-distribution} shows this, the higher temperature steady state produces a flattened, wider distribution of energies than the low temperature, almost ground state.

\subsection{Finding the critical temperature}
We want to solve eq (\ref{eq:critical_temp}) to find the critical temperature of the Ising model in two dimensions for a general lattice size $L$. To do so, we first need to find the constant $a$. As seen from (\ref{eq:a}), we need to find the approximate critical temperature of two finite lattices and then make an estimate of $a$, using $\nu = 1$. Since we have done simulations over four finite lattice sizes, we will find an average value of $a$ by calculating over all possible combinations of lattices, but first we need to determine where the critical temperature is for the different lattices. We know that the theoretical critical temperature $T_C(L = \infty) \approx 2.269$ as given by Lars Onsager \cite{Onsager1944}, and we can see in our plots of the different sized $L$'s in [\ref{fig:parallelisation}] that in the $T \in [2.250, 2.30]$ range something happens with our model. If we focus in on the plot over the specific heat $C_v$, we can read of the values of $T$ that maximize $C_v$. We use $C_v$ because we know that as the system approaches the critical temperature, the heat capacity increases as $T \rightarrow T_C \rightarrow C_v \rightarrow \infty$. It is also clear that the values of $C_v$ are slightly more stable than e.g the susceptibility $\chi$, which means that the results we find will be less prone to numerical errors and instabilities. 

When we now calculate the value of $a$, we find that $a = -0.6277 \hdots \approx -\frac{2}{3}$. Inserting this value into eq (\ref{eq:critical_temp}) for each $L$ and then calculating the average gives us the critical temperature $T_C(L = \infty) \approx 2.273$. This gives us a difference of $4 \cdot 10^{-3}$ and a percent error of $\delta = 0.17\%$ compared to the analytical solution. This is quite a good result, but could most likely be improved by simply running simulations of even more lattices and and even higher resolution. It is also quite clear that some computation time was wasted in the range of temperatures near $T = 2.0$. As there is nothing of particular value to be found here, at least for the case of finding the critical temperature, it would prove more efficient to narrow the range of $T$ to something like $T \in [2.20, 2.30]$ and then increase the resolution to $\Delta T \leq 0.001$. Finally, it would increase our accuracy to run each temperature over more cycles than $1\text{E}6$, however as the cycles increase the returns are diminishing compared to the compute time required.

\iffalse
This can be done by calculating (\ref{eq:a}). If we set $L_1 = 40$ and $L_2 = 100$ then we can find the estimates for $T(L_1)$ and $T(L_2)$ by looking at the behavior of the plots in \ref{fig:parallelisation}. It is clear that around the $T \in [2.26, 2.29]$ range, something is happening in the system. Although it would be possible to read values from all of the plots, some of them are more likely to produce errors than others. This is mainly the case for the susceptibility, as it is clearly unstable when approaching the critical temperature. The plot of the specific heat on the other hand, is much more stable and should thus give us a more accurate reading for the critical temperatures for the given lattices. We read the temperature at the where the specific heat is at its highest. We can thus approximate that $T_C(L_1 = 100) \approx 2.29$ and $T_C(L_2 = 40) \approx 2.28$. We can now insert this into eq (\ref{eq:critical_temp}) using $\nu = 1$, which gives us the critical temperature of an infinite lattice as $T_C(L = \infty) \approx 2.283$, which is approximately $0.014$ bigger then the value $T_C(L = \infty) \approx 2.269$ found by Lars Onager \cite{Onsager1944}. It would be possible to get an even better approximation, however, this would require an even smaller $\Delta T$ than what was used to produce \ref{fig:parallelisation}. Ideally, we would do an average over $a$ for all possible combinations of $T_C(L)$, however, due to our low resolution several of the peaks appear at the same temperature, which by \cref{eq:a} leads to $a = 0$, such that $T_C(L = \infty) = T_C(L)$ for all $L$.
\fi

\subsection{Parallelization}
Simulating the Ising model for two dimensions is quite a slow process if done on only one CPU core. It is therefore of great interest to share some of the burden from one core to several. We see from \ref{tab:timing} that when our code runs on several cores that the speed of our code increases by approximately a factor of $2$. This is not really unexpected as one would assume that many cores would be better than one, and that is in fact what we have observed. We implemented parallelization using the \href{https://github.com/open-mpi/ompi}{OpenMPI} library, running on four physical cores. It does not seem like OpenMPI supports Hyperthreading/non - physical cores without some extra tinkering on our part so there might have been more performance to gain, although using non-physical cores could also be less efficient given the tasks we were running.    

\end{document}
